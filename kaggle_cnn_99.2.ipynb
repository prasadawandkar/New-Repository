{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data.iloc[:, 1:].values.reshape((42000,28,28,1))\n",
    "labels = data.iloc[:, 0].values.reshape((42000,1))\n",
    "test = test.values.reshape((28000,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):    \n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0))\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = zero_pad(features, 2)\n",
    "test = zero_pad(test, 2)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features,\n",
    "                                                   labels,\n",
    "                                                   test_size = 0.025,\n",
    "                                                   random_state = 0,\n",
    "                                                   stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize image inputs\n",
    "def feature_norm(x):\n",
    "    \n",
    "    X_norm = x / 255\n",
    "    return X_norm\n",
    "\n",
    "X_train, X_test = feature_norm(X_train), feature_norm(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One-hot-code y values\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "Y_train, Y_test = convert_to_one_hot(Y_train, 10), convert_to_one_hot(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape = (None, n_H0, n_W0, n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape = (None, n_y))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X, Y = create_placeholders(32, 32, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(C_F1 = 6, C_F3 = 16):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        C_W1 : [5, 5, 1, 16]\n",
    "                        C_W3 : [5, 5, 6, 32]\n",
    "                        C_W5 : [5, 5, 16, 120]\n",
    "                        FC => shape    [1, 120]\n",
    "                        FC_W6: [120, 84]\n",
    "                        FC_W7: [84, 10]\n",
    "                        \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    C_W1 = tf.get_variable(\"C_W1\", [5, 5, 1, C_F1], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    C_W3 = tf.get_variable(\"C_W3\", [5, 5, C_F1, C_F3], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    C_W5 = tf.get_variable(\"C_W5\", [5, 5, C_F3, 120], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    FC_W6 = tf.get_variable(\"FC_W6\", [120, 84], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    bias_6 = tf.Variable(tf.zeros([84]))\n",
    "    FC_W7 = tf.get_variable(\"FC_W7\", [84, 10], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    bias_7 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    parameters = {\"C_W1\": C_W1,\n",
    "                  \"C_W3\": C_W3,\n",
    "                  \"C_W5\": C_W5,\n",
    "                  \"FC_W6\": FC_W6,\n",
    "                  \"bias_6\": bias_6,\n",
    "                  \"FC_W7\": FC_W7,\n",
    "                  \"bias_7\": bias_7}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, pooling = 'max'):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D_1 -> RELU -> MAX/AVGPOOL_2 -> CONV2D_3 -> RELU -> MAX/AVGPOOL_4 -> CONV2D_5 -> RELU -> FLATTEN -> \n",
    "    FULLYCONNECTED_6 -> FULLYCONNECTED_7\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z7 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    C_W1 = parameters['C_W1']\n",
    "    C_W3 = parameters['C_W3']\n",
    "    C_W5 = parameters['C_W5']\n",
    "    FC_W6 = parameters['FC_W6']\n",
    "    bias_6 = parameters['bias_6']\n",
    "    FC_W7 = parameters['FC_W7']\n",
    "    bias_7 = parameters['bias_7']\n",
    "    \n",
    "    # CONV2D: filters C_W1, stride of 1, padding 'SAME'\n",
    "    conv_1 = tf.nn.conv2d(X, C_W1, strides = [1,1,1,1], padding = 'VALID') #[m,h,w,c]\n",
    "    #print(conv_1)\n",
    "    \n",
    "    # padding arguement refer to \n",
    "    # https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(conv_1)\n",
    "\n",
    "    if pooling == 'max':\n",
    "        # MAXPOOL: window 2x2, stride 2, padding 'SAME'\n",
    "        maxpool_2 = tf.nn.max_pool(A1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        #print(maxpool_2)\n",
    "    \n",
    "        # CONV2D: filters C_W3, stride 1, padding 'SAME'\n",
    "        conv_3 = tf.nn.conv2d(maxpool_2, C_W3, strides = [1,1,1,1], padding = 'VALID')\n",
    "        #print(conv_3)\n",
    "    \n",
    "        # RELU\n",
    "        A2 = tf.nn.relu(conv_3)\n",
    "        # MAXPOOL: window 2x2, stride 2, padding 'SAME'\n",
    "        maxpool_4 = tf.nn.max_pool(A2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        #print(maxpool_4)\n",
    "    \n",
    "        # CONV2D: filters C_W5, stride 1, padding 'SAME'\n",
    "        conv_5 = tf.nn.conv2d(maxpool_4, C_W5, strides = [1,1,1,1], padding = 'VALID')\n",
    "        #print(conv_5)\n",
    "        \n",
    "    elif pooling == 'avg':\n",
    "        # AVGPOOL: window 2x2, stride 2, padding 'SAME'\n",
    "        avgpool_2 = tf.nn.avg_pool(A1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        #print(maxpool_2)\n",
    "    \n",
    "        # CONV2D: filters C_W3, stride 1, padding 'SAME'\n",
    "        conv_3 = tf.nn.conv2d(avgpool_2, C_W3, strides = [1,1,1,1], padding = 'VALID')\n",
    "        #print(conv_3)\n",
    "    \n",
    "        # RELU\n",
    "        A2 = tf.nn.relu(conv_3)\n",
    "        # AVGPOOL: window 2x2, stride 2, padding 'SAME'\n",
    "        avgpool_4 = tf.nn.avg_pool(A2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "        #print(maxpool_4)\n",
    "    \n",
    "        # CONV2D: filters C_W5, stride 1, padding 'SAME'\n",
    "        conv_5 = tf.nn.conv2d(avgpool_4, C_W5, strides = [1,1,1,1], padding = 'VALID')\n",
    "        #print(conv_5)\n",
    "    \n",
    "    # RELU\n",
    "    A3 = tf.nn.relu(conv_5)\n",
    "    # FLATTEN\n",
    "    fc_5 = tf.reshape(A3, [-1, 120]) #output: [batch_size, k]\n",
    "    # FULLY-CONNECTED\n",
    "    fc_6 = tf.nn.bias_add(tf.matmul(fc_5, FC_W6), bias_6)\n",
    "    # RELU\n",
    "    A4 = tf.nn.relu(fc_6)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    fc_7 = tf.nn.bias_add(tf.matmul(A4, FC_W7), bias_7)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return fc_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(fc_7, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    fc_7 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples, 10)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = fc_7, labels = Y)) #squeeze and compute mean\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, test, learning_rate = 0.0005,\n",
    "          num_epochs = 30, minibatch_size = 64, pooling = 'max',\n",
    "          C_F1 = 6, C_F3 = 16,\n",
    "          print_cost = True, learning_decay = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D_1 -> RELU -> MAXPOOL_2 -> CONV2D_3 -> RELU -> MAXPOOL_4 -> CONV2D_5 -> RELU -> FLATTEN -> \n",
    "    FULLYCONNECTED_6 -> FULLYCONNECTED_7\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 28, 28, 1)\n",
    "    Y_train -- test set, of shape (None, n_y = 10)\n",
    "    X_test -- training set, of shape (None, 28, 28, 1)\n",
    "    Y_test -- test set, of shape (None, n_y = 10)\n",
    "    test -- the test set (to be predicted), of shape (None, 28, 28, 1)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    pooling -- pooling method, avg or max\n",
    "    C_F1 -- filters number in conv_1\n",
    "    C_F3 -- filters number in conv_3\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(5)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 7                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(C_F1 = C_F1, C_F3 = C_F3)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    fc_7 = forward_propagation(X, parameters, pooling = pooling)\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(fc_7, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                #print(temp_cost)\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0 and epoch / 5 > 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "                if learning_decay:\n",
    "                    learning_rate /= 2\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "#         plt.plot(np.squeeze(costs))\n",
    "#         plt.ylabel('cost')\n",
    "#         plt.xlabel('iterations (per tens)')\n",
    "#         plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#         plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(fc_7, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        \n",
    "        #output the prediction for kaggle\n",
    "        predicted_labels = np.zeros(test.shape[0])\n",
    "        predicted_labels = sess.run(predict_op, feed_dict={X: test})\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 5: 0.025987\n",
      "Cost after epoch 10: 0.010858\n",
      "Cost after epoch 15: 0.004890\n",
      "Cost after epoch 20: 0.005837\n",
      "Cost after epoch 25: 0.002374\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters, predicted_labels = model(X_train, Y_train, X_test, Y_test, test, learning_rate = 0.0004,\n",
    "                                          num_epochs = 30, minibatch_size = 64, pooling = 'max',\n",
    "                                          C_F1 = 16, C_F3 = 64,\n",
    "                                          print_cost = True, learning_decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submissions = pd.DataFrame({'ImageId': np.arange(1 , 1 + test.shape[0]), 'Label': predicted_labels.astype(int)})\n",
    "submissions.to_csv('./tf_CNN(more_filters).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
